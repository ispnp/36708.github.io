
<!DOCTYPE HTML>
<!--
	Prologue by HTML5 UP
	html5up.net | @n33co
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>ABCDE ML</title>
		<meta charset="utf-8" />
		
	</head>
<body>

	<h1>36-708: The ABCDE of Statistical Methods in Machine Learning</h1>
	<h2>(aka: developing judgment for complex stat-ml methods)</h2>

<h2>Class location and time: DH 1211 (MW 10:30-11:50am)</h2>
	
<h3> Office hours locations and time: Aaditya: BH 132H (MW 12-12:30pm), Pratik: GHC 8106 (T 2-3pm)</h3>

<b>Course details:</b><br>
<ul>
<li> <a href="course_information.html">Overview</a><br>
<li> <a href="Syllabus.pdf">Syllabus</a><br><br>
</ul>
	

<b>Scribes:</b><br>

<ul>
<li> <a href="https://docs.google.com/spreadsheets/d/1m4uRV-lpLqNeFVz6dLczAQ4O6WMuQc1tg1AHvkMHc8o/edit#gid=0">Crowd-scribing sign-up sheet (access with a CMU account)</a><br>
<li> <a href="https://www.overleaf.com/read/jzhmrrfgfnkh">Crowd-scribed class notes (read-only; ask for edit link if in class)</a><br><br>	
</ul>

<b>Homeworks:</b><br>
	
<ul>
<li> <a href="https://www.dropbox.com/s/ggxiqnojsoi4er3/homework_1.pdf?dl=0">Homework 1</a><br>
<li> <a href="https://www.dropbox.com/s/0hba8tiaguuqoqa/homework_2.pdf?dl=0">Homework 2</a><br>
<li> <a href="https://www.dropbox.com/s/649acnti76k2gbo/homework_3.pdf?dl=0">Homework 3</a><br>
<li> <a href=""><font color=red>Homework 4</font></a><br><br>
</ul>
	
<b>Lectures:</b><br>

<ul>

<p>
<li> L01 (Jan 13): Introduction <br>

	<ul>
	<li> <a href="https://www.dropbox.com/s/nedypj53c98c7tf/Syllabus.pdf?dl=0">Course syllabus</a>
	</ul>

<p>	
<li> L02 (Jan 15): Basics of supervised learning: regression, classification [A, B, C, D, E] <br>

	<ul>
	<li> <a href="https://www.dropbox.com/s/lz27mdj2a9a1lj7/lecture_02.pdf?dl=0">Scribe note (Allie Del Giorno)</a>
	<li> <a href="https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf#page=20">Overview of supervised learning (Hastie, Tibshirani, Friedman, 2017) [Elements of statistical learning, chapter 01]</a>
	<li> <a href="http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf#page=28">Statistical learning (Jamse, Witten, Hastie, Tibshirani, 2017) [An introduction statistical learning, chapter 02]</a>
	
	
	</ul>

<p>
<li> L-- (Jan 20): No class (MLK day) <br>


<p>
<li> L03 (Jan 22): Nearest-neighbor methods: k-nn regression and classification [B] <br>

	<ul>
	<li> <a href="https://www.dropbox.com/s/swfsro95xmdfuvr/lecture_03.pdf?dl=0">Scribe note (Yunhan Wen)</a>
	<li> <a href="https://link.springer.com/book/10.1007/978-3-319-25388-6">Lectures on the nearest neighbor method (Blau, Devroye, 2015)</a>
	<li> <a href="https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf#page=478">Prototype methods and nearest-neighbors (Hastie, Tibshirani, Friedman, 2017) [Elements of statistical learning, chapter 13]</a>
	<li> <a href="https://ieeexplore.ieee.org/abstract/document/1053964">Nearest neighbor pattern classification (Cover, Hart, 1967)</a>
	<li> <a href="https://projecteuclid.org/euclid.aos/1176347757">Kernel and nearest-neighbor estimation of a conditional quantile (Bhattacharya, Gangopadhyay, 1990)</a>
	</ul>

<p>
<li> L04 (Jan 27): Predictive inference: conformal prediction [C, E] <br>

	<ul>
	<li> <a href="https://www.dropbox.com/s/5grfh5jwobmmjg2/lecture_04.pdf?dl=0">Scribe note (Ian Waudby-Smith)</a>
	<li> <a href="http://jmlr.csail.mit.edu/papers/volume9/shafer08a/shafer08a.pdf">Conformal prediction (Vovk, 2005) [Algorithmic learning in a random world, chapter 02]</a>
	<li> <a href="http://jmlr.csail.mit.edu/papers/volume9/shafer08a/shafer08a.pdf">A tutorial on conformal prediction (Shafer, Vovk, 2008)</a>
	<li> <a href="https://arxiv.org/pdf/1604.04173.pdf">Distribution-free predictive inference for regression (Lei, G'Sell, Rinaldo, Tibshirani, Wasserman, 2017)</a>
	</ul>

<p>
<li> L05 (Jan 29): Ensemble methods: boosting (game-theoretic perspective) [A] <br>
	<ul>
	<li> <a href="https://www.dropbox.com/s/rtom5jhdpzltvw4/lecture_05.pdf?dl=0">Scribe note (Sasha Podkopaev)</a>
	<li> <a href="https://mitpress.ublish.com/ereader/7093/?preview=#page/145">Boosting (Mohri, Rostamizadeh, Talwalkar, 2018) [Foundations of machine learning, chapter 07]</a>
	<li> <a href="http://www.cs.cmu.edu/~ninamf/LGO10/wm.pdf">The strength of weak learnability (Schapire, 1990)</a>
	<li> <a href="https://pdfs.semanticscholar.org/d620/946e24eee13bc3bdd5ceb0f90a3dc4bc4a54.pdf">Boosting a weak learning algorithm by majority (Freund, 1995)</a>
	<li> <a href="http://www.cs.cmu.edu/~ninamf/LGO10/wm.pdf">The weighted majority algorithm (Littlestone, Warmuth, 1992)</a>
	<li> <a href="https://pdfs.semanticscholar.org/5fb5/f7b545a5320f2a50b30af599a9d9a92a8216.pdf">A decision-theoretic generalization of on-line learning and an application to boosting (Freund, Schapire, 1997)</a>
	</ul>

<p>
<li> L06 (Feb 03): Ensemble methods: boosting (statistical perspective) [A] <br>
	<ul>
	<li> <a href="https://www.dropbox.com/s/6mwtmfkf6voj4kf/lecture_06.pdf?dl=0">Scribe note (Weichen Wu)</a>
	<li> <a href="https://mitpress.ublish.com/ereader/7093/?preview=#page/145">Boosting (Foundations of machine learning, chapter 07)</a>
	<li> <a href="http://papers.neurips.cc/paper/1737-potential-boosters.pdf">Potential boosters (Duffy, Helmbold, 1999)</a>	
	<li> <a href="https://papers.nips.cc/paper/1766-boosting-algorithms-as-gradient-descent.pdf">Boosting algorithms as gradient descnet (Mason, Baxter, Bartlett, Frean, 1999)</a>
	<li> <a href="https://projecteuclid.org/download/pdf_1/euclid.aos/1013203451">Greedy function approximation: a gradient boosting machine (Friedman, 2001)</a>	
	</ul>

<p>
<li> L07 (Feb 05): Ensemble methods: boosting (computational considerations, applications) [C, D], guest lecture by Allie <br>
	<ul>
	<li> <a href="https://www.dropbox.com/s/9p59gve9nxu1qfl/lecture_07.pdf?dl=0">Scribe note (Tuhinangshu Choudhury)</a>
	<li> <a href="https://www.dropbox.com/s/9p59gve9nxu1qfl/lecture_07.pdf?dl=0">SpeedBoost: anytime prediction with uniform near-optimality (Grubb, Bagnell, 2012)</a>
	</ul>

<p>
<li> L08 (Feb 10): Ensemble methods: boosting (generalization) [B] <br>
	<ul>
	<li> <a href="https://www.dropbox.com/s/r5u2kfh7uvhw3hn/lecture_08.pdf?dl=0">Scribe note (Rajshekar Das)</a>
	<li> <a href="https://mitpress.ublish.com/ereader/7093/?preview=#page/145">Boosting (Foundations of machine learning, chapter 07)</a>
	<li> <a href="https://projecteuclid.org/download/pdf_1/euclid.aos/1024691352">Boosting the margin: a new explanation for the effectiveness of voting methods</a>
	</ul>

<p>
<li> L09 (Feb 12): Quiz 1  <br>
	<ul>
	<li> Topics: basics (supervised learning), prototype methods (nearest-neighbor methods), predictive inference (conformal prediction), ensemble methods (boosting)
	</ul>

<p>
<li> L10 (Feb 17): Ensemble methods: bagging, random forests [A, B] <br>
	<ul>
	<li> <a href="https://www.dropbox.com/s/w1cwg68uzea1u3s/lecture_10.pdf?dl=0">Scribe note (Andrew Warren)</a>
	<li> <a href="https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf#page=238">Random forests (Hastie, Tibshirani, Friedman, 2017) [Elements of statistical learning, chapter 15]</a>
	<li> <a href="http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf#page=186">Tree-based methods (Jamse, Witten, Hastie, Tibshirani, 2017) [An introduction statistical learning, chapter 08]</a>
	<li> <a href="https://www.stat.berkeley.edu/~breiman/bagging.pdf">Bagging predictors (Leo Breiman, 1994)</a>
	</ul>

<p>
<li> L11 (Feb 19): Variable importance: random forests case study [C, E] <br>
	<ul>
	<li> <a href="https://www.dropbox.com/s/d4y1d6dj3evib7i/lecture_11.pdf?dl=0">Scribe note (Amanda Coston)</a>
	<li> <a href="https://ieeexplore.ieee.org/abstract/document/598994">Random decision forests (Ho, 1995)</a>
	<li> <a href="https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf">Random forests (Breiman, 2001)</a>
	<li> Additional reading: <a href="https://arxiv.org/pdf/1905.03151.pdf">Phase stop permuting features: an explanation and alternatives (Hooker, Mentch, 2019)</a>
	<li> Additional reading: <a href="https://arxiv.org/pdf/2003.03629.pdf">Getting better from worse: augmented bagging and a cautionary tale of variable importance (Mentch, Zhou, 2020)</a>
	</ul>

<p>
<li> L12 (Feb 24): Datapoint importance: Shapley values [C, E] <br>
	<ul>
	<li> <a href="https://www.dropbox.com/s/v0gsrz604vhsyl9/lecture_12.pdf?dl=0">Scribe note (Zeyu Tang)</a>
	<li> <a href="https://www.rand.org/content/dam/rand/pubs/research_memoranda/2008/RM670.pdf">Notes on n-person game -- II: The value of an n-person game (Shapley, 1951)</a>
	<li> <a href="https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions">A unified approach to interpreting model predictions (Lundberg, Lee, 2017)</a>
	<li> <a href="https://arxiv.org/abs/1904.02868">Data Shapley: Equitable valuation of data for machine learning (Ghorbani, Zou, 2019)</a>
	<li> <a href="https://christophm.github.io/interpretable-ml-book/shapley.html">Shapley values (Molnar, 2020) [Interpretable machine learning, chapter 05]</a>
	<li> <a href="https://arxiv.org/pdf/2002.11097.pdf">Problems with Shapley-value-based explanations as feature importance measures (Kumar, Venkatasubramanian, Scheidegger, Friedler, 2020)</a>
	</ul>

<p>
<li> L13 (Feb 26): Ensemble methods: stacking [A, B] <br>
	<ul>
	<li> <a href="https://www.dropbox.com/s/u77y14gjtjdve5d/lecture_13.pdf?dl=0">Scribe note (Don Dennis)</a>
	<li> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0893608005800231">Stacked generalization (Wolpert, 1992)</a>
	<li> <a href="https://statistics.berkeley.edu/sites/default/files/tech-reports/367.pdf">Stacked regressions (Breiman, 1996)</a>

	</ul>
	
<p>
<li> L14 (Mar 02): Predictive inference: jackknife+ [C, E] <br>
	<ul>
	<li> <a href="https://www.dropbox.com/s/1m8z6lex1tb5sly/lecture_14.pdf?dl=0">Scribe note (Max Rubinstein)</a>
	<li> <a href="https://arxiv.org/abs/1905.02928">Predictive inference with the jackknife+ (Barber, Candes, Ramdas, Tibshirani, 2019)</a>
	<li> <a href="https://arxiv.org/abs/2002.09025">Predictive inference is free with the jackknife+after-bootstrap (Kim, Xu, Barber, 2020)</a>

	
	</ul>

<p>
<li> L15 (Mar 04): Predictive inference: leave-one-out [C, E] <br>
	<ul>
	<li> <a href="https://www.dropbox.com/s/knd5epjpa67mwrf/lecture_15.pdf?dl=0">Scribe note (Naveen Basavaraj)</a>
	<li> <a href="https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf#page=238">Model assessment and selection (Hastie, Tibshirani, Friedman, 2017) [Elements of statistical learning, chapter 07]</a>
	<li> <a href="http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf#page=186">Resampling methods (James, Witten, Hastie, Tibshirani, 2017) [An introduction statistical learning, chapter 05]</a>
	</ul>

<p>
<li> L-- (Mar 09): No class (spring break)  <br>


<p>
<li> L-- (Mar 11): No class (spring break) <br>

<p>
<li> L16 (Mar 16): No class (no class due to COVID-19 online shift) <br>

<p>
<li> L17 (Mar 18): Mid-class reap: (methods/rows) k-nearest-neighbor; boosting; bagging; random forest; stacking; (aspects/columns) algorithms; bias-variance; computation, conformal; (practical) data aspects; explainability, interpretability <br>

	<ul>
	<li> <a href="https://www.dropbox.com/s/408d5i73kadl5vu/lecture_16.pdf?dl=0">Scribe note (Lucio Dery)</a>
	<li> References:
	</ul>

<p>
<li> L18 (Mar 23):  Quiz 2 <br>

	<ul>
	<li> Topics: variable importance (random forest), data importance (Shapley values), ensemble methods (stacking), predictive inference (jackknife+, leave-one-out)
	</ul>

<p>
<li> L19 (Mar 25): Kernel basics  <br>

	<ul>
	<li> <a href="https://www.dropbox.com/s/d6knu07mthgckim/lecture_19.pdf?dl=0">Scribe note (Ankur Mallick)</a>
	<li> <a href="https://mitpress.ublish.com/ereader/7093/?preview=#page/105">Kernel methods (Foundations of machine learning, chapter 06)</a>
	<li> <a href="http://mlss.tuebingen.mpg.de/2015/slides/gretton/part_1.pdf">Introduction to RKHS (Gretton, 2015)</a>
	</ul>

<p>
<li> L20 (Mar 30): Kernel basics <br>

	<ul>
	<li> <a href="https://www.dropbox.com/s/tythg2o8rgxgk2v/lecture_20.pdf?dl=0">Scribe note (Lorenzo Tomaselli)</a>
	<li> <a href="https://mitpress.ublish.com/ereader/7093/?preview=#page/105">Kernel methods (Foundations of machine learning, chapter 06)</a>
	<li> <a href="http://mlss.tuebingen.mpg.de/2015/slides/gretton/part_1.pdf">Mappings of Probabilities to RKHS and applications (Gretton, 2015)</a>
	</ul>

<p>
<li> L21 (Apr 01): Kernel basics <br>

	<ul>
	<li> <a href="https://www.dropbox.com/s/qvk8dw56qz0khg5/lecture_21.pdf?dl=0">Scribe note (Nick Kissel)</a>
	<li> <a href="https://mitpress.ublish.com/ereader/7093/?preview=#page/105">Kernel methods (Foundations of machine learning, chapter 06)</a>
	<li> <a href="http://mlss.tuebingen.mpg.de/2015/slides/gretton/part_1.pdf">Mappings of Probabilities to RKHS and applications (Gretton, 2015)</a>
	
	</ul>

<p>
<li> L22 (Apr 06): Kernel regression, kernel classification <br>

	<ul>
	<li> <a href="https://www.dropbox.com/s/v9a85efiyy7g495/lecture_22.pdf?dl=0">Scribe note (Mike Stanley)</a>
	<li> <a href="https://mitpress.ublish.com/ereader/7093/?preview=#page/267">Regression (Foundations of machine learning, chapter 11)</a>
	<li> <a href="http://mlss.tuebingen.mpg.de/2015/slides/gretton/part_3.pdf">Dependence measures using RKHS embeddings (Gretton, 2015)</a>
	</ul>

<p>
<li> L23 (Apr 08): Unsupervised learning (clustering, dimensionality reduction) <br>

	<ul>
	<li> <a href="">Scribe note (Saharsh Agarwal)</a>
	<li> <a href="https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf#page=504">Unsupervised learning (Hastie, Tibshirani, Friedman, 2017) [Elements of statistical learning, chapter 14]</a>
	<li> <a href="http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf">k-means++: the advantage of careful seeding (Arthur, Vassilvitskii, 2006)</a>
	</ul>
	
<p>
<li> L24 (Apr 13): Unsupervised learning (dimensionality reduction) <br>

	<ul>
	<li> <a href="">Scribe note (Misha Khodak)</a>
	<li> <a href="https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf#page=504">Unsupervised learning (Hastie, Tibshirani, Friedman, 2017) [Elements of statistical learning, chapter 14]</a>
	<li> <a href="http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf#page=381">Resampling methods (James, Witten, Hastie, Tibshirani, 2017) [An introduction statistical learning, chapter 05]</a>
	<li> <a href="https://sebastianraschka.com/Articles/2014_kernel_pca.html">Kernel tricks and nonlinear dimensionality reduction via RBF kernel PCA (Raschka, 2014) </a>
	<li> <a href="https://arxiv.org/pdf/1207.3538.pdf">Kernel Principal Component Analysis and its Applications in Face Recognition and Active Shape Models (Wang, 2012)</a>
	</ul>
	
<p>
<li> L25 (Apr 15):  Unsupervised learning (dimensionality reduction) <br>

	<ul>
	<li> <a href="https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf#page=504">Unsupervised learning (Hastie, Tibshirani, Friedman, 2017) [Elements of statistical learning, chapter 14]</a>
	<li> <a href="http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf#page=381">Unsupervised learning (James, Witten, Hastie, Tibshirani, 2017) [An introduction to statistical learning, chapter 10]</a>
	<li> <a href="https://arxiv.org/pdf/1501.03796.pdf">The Fast Convergence of Incremental PCA (Balsubramani, Dasgupta, Freund, 2015)</a>
	<li> <a href="https://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf">Extracting and Composing Robust Features with Denoising Autoencoders (Vincent, Larochelle, Bengion, Manzagol, 2008) </a>

	</ul>

<p>
<li> L26 (Apr 17): Guest lecture by <a href="http://lucasmentch.com/index.html">Prof. Lucas Mentch</a> <br>

	<ul>
	<li> Topics:
	</ul>
	
<p>
<li> L27 (Apr 20):  <br>

	<ul>
	<li> <a href="">Scribe note (Jenn Williams)</a>
	<li> References:
	</ul>
	
<p>
<li> L28 (Apr 22):  Quiz 3 <br>

	<ul>
	<li> Topics: kernels (basics, regression, classification), unsupervised learning (clustering, PCA, kernel PCA, stochastic PCA, deep PCA, autoencoders)   
	</ul>
	
<p>
<li> L29 (Apr 27):  <br>

	<ul>
	<li> <a href="">Scribe note (Zeyu Tang)</a>
	<li> References:
	</ul>
	
<p>
<li> L30 (Apr 29):  <br>
	<ul>
	<li> <a href="">Scribe note (Dhivya Eswaran)</a>
	<li> References:
	</ul>
	
<br>
</ul>

<b>Some textbooks:</b> <br>
<ul>
<li><a href="https://web.stanford.edu/~hastie/ElemStatLearn/">The Elements of Statistical Learning: Data Mining, Inference and Prediction.</a> Trevor Hastie, Robert Tibshirani, Jerome Friedman.<br>
<li><a href="http://faculty.marshall.usc.edu/gareth-james/ISL/">An Introduction to Statistical Learning: With Applications in R.</a> Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani.<br>
<li><a href="https://cs.nyu.edu/~mohri/mlbook/">Foundations of Machine Learning.</a> Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.<br><br>
</ul>


<b>References to catch up on prerequisites:</b>
<ul>
  <li><a href=https://www.youtube.com/channel/UCu8Pv6IJsbQdGzRlZ5OipUg/videos>Videos of Larry Wasserman's 36-705 course</a><br>
  <li><a href="http://www.cs.cmu.edu/~zkolter/course/linalg/index.html">Linear algebra review</a>, videos by Zico Kolter<br>
  <li><a href="https://www.youtube.com/channel/UC7gOYDYEgXG1yIH_rc2LgOw/playlists">Real analysis, calculus, and more linear algebra</a>, videos by Aaditya Ramdas <br>
  <li><a href="prerequisite_topics.pdf">Convex optimization prequisites review</a> from Spring 2015 course, by Nicole Rafidi<br>
  <li>See also Appendix A of <a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Boyd and Vandenberghe (2004) for general mathematical review</a><br><br>
</ul>
	  
<b>Potentially useful resources available online (not necessarily verified by the course staff):</b>
<ul>
  <li><a href="https://github.com/josephmisiti/awesome-machine-learning/blob/master/books.md">A list of machine learning books</a><br>
  <li><a href="http://ciml.info/">A course in machine learning</a><br>
   <li><a href="https://christophm.github.io/interpretable-ml-book/">Interpretable Machine Learning</a><br>
   <li><a href="https://fairmlbook.org/">Fairness and machine learning</a><br><br>
</ul>

</body>

</html>
