
<!DOCTYPE HTML>
<!--
	Prologue by HTML5 UP
	html5up.net | @n33co
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>ABCDE ML</title>
		<meta charset="utf-8" />
		
	</head>
<body>

	<h1>36-708: The ABCDE of Statistical Methods in Machine Learning</h1>
	<h2>(aka: developing judgment for complex stat-ml methods)</h2>

<h2>Class location and time: DH 1211 (MW 10:30-11:50am)</h2>
	
<h3> Office hours locations and time: Aaditya: BH 132H (MW 12-12:30pm), Pratik: GHC 8106 (T 2-3pm)</h3>

<b>Course details:</b><br>
<ul>
<li> <a href="course_information.html">Overview</a><br>
<li> <a href="Syllabus.pdf">Syllabus</a><br><br>
</ul>
	

<b>Scribes:</b><br>

<ul>
<li> <a href="https://docs.google.com/spreadsheets/d/1m4uRV-lpLqNeFVz6dLczAQ4O6WMuQc1tg1AHvkMHc8o/edit#gid=0">Crowd-scribing sign-up sheet (access with a CMU account)</a><br>
<li> <a href="https://www.overleaf.com/read/jzhmrrfgfnkh">Crowd-scribed class notes (read-only; ask for edit link if in class)</a><br><br>	
</ul>

<b>Homeworks:</b><br>
	
<ul>
<li> <a href="https://www.dropbox.com/s/ggxiqnojsoi4er3/homework_1.pdf?dl=0">Homework 1</a><br>
<li> <a href="https://www.dropbox.com/s/0hba8tiaguuqoqa/homework_2.pdf?dl=0">Homework 2</a><br>
<li> <a href="https://www.dropbox.com/s/649acnti76k2gbo/homework_3.pdf?dl=0">Homework 3</a><br>
<li> <a href="https://www.dropbox.com/s/gfeegtxts76l4ol/homework_4.pdf?dl=0">Homework 4</a><br><br>
</ul>
	
<b>Lectures:</b><br>

<ul>

<p>
<li> L01 (Jan 13): Introduction <br>

	<ul>
	<li> <a href="https://www.dropbox.com/s/nedypj53c98c7tf/Syllabus.pdf?dl=0">Course syllabus</a>
	</ul>

<p>	
<li> L02 (Jan 15): Basics of supervised learning: regression, classification [A, B, C, D, E] <br>

	<ul>
	<li> <a href="https://www.dropbox.com/s/lz27mdj2a9a1lj7/lecture_02.pdf?dl=0">Scribe note (Allie Del Giorno)</a>
	<li> <a href="https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf#page=20">Overview of supervised learning (Hastie, Tibshirani, Friedman, 2017) [Elements of statistical learning, chapter 01]</a>
	<li> <a href="http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf#page=28">Statistical learning (Jamse, Witten, Hastie, Tibshirani, 2017) [An introduction statistical learning, chapter 02]</a>
	
	
	</ul>

<p>
<li> L-- (Jan 20): No class (MLK day) <br>


<p>
<li> L03 (Jan 22): Nearest-neighbor methods: k-nn regression and classification [B] <br>

	<ul>
	<li> <a href="https://www.dropbox.com/s/swfsro95xmdfuvr/lecture_03.pdf?dl=0">Scribe note (Yunhan Wen)</a>
	<li> <a href="https://link.springer.com/book/10.1007/978-3-319-25388-6">Lectures on the nearest neighbor method (Blau, Devroye, 2015)</a>
	<li> <a href="https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf#page=478">Prototype methods and nearest-neighbors (Hastie, Tibshirani, Friedman, 2017) [Elements of statistical learning, chapter 13]</a>
	<li> <a href="https://ieeexplore.ieee.org/abstract/document/1053964">Nearest neighbor pattern classification (Cover, Hart, 1967)</a>
	<li> <a href="https://projecteuclid.org/euclid.aos/1176347757">Kernel and nearest-neighbor estimation of a conditional quantile (Bhattacharya, Gangopadhyay, 1990)</a>
	</ul>

<p>
<li> L04 (Jan 27): Predictive inference: conformal prediction [C, E] <br>

	<ul>
	<li> <a href="https://www.dropbox.com/s/5grfh5jwobmmjg2/lecture_04.pdf?dl=0">Scribe note (Ian Waudby-Smith)</a>
	<li> <a href="http://jmlr.csail.mit.edu/papers/volume9/shafer08a/shafer08a.pdf">Conformal prediction (Vovk, 2005) [Algorithmic learning in a random world, chapter 02]</a>
	<li> <a href="http://jmlr.csail.mit.edu/papers/volume9/shafer08a/shafer08a.pdf">A tutorial on conformal prediction (Shafer, Vovk, 2008)</a>
	<li> <a href="https://arxiv.org/pdf/1604.04173.pdf">Distribution-free predictive inference for regression (Lei, G'Sell, Rinaldo, Tibshirani, Wasserman, 2017)</a>
	</ul>

<p>
<li> L05 (Jan 29): Ensemble methods: boosting (game-theoretic perspective) [A] <br>
	<ul>
	<li> <a href="https://www.dropbox.com/s/rtom5jhdpzltvw4/lecture_05.pdf?dl=0">Scribe note (Sasha Podkopaev)</a>
	<li> <a href="https://mitpress.ublish.com/ereader/7093/?preview=#page/145">Boosting (Mohri, Rostamizadeh, Talwalkar, 2018) [Foundations of machine learning, chapter 07]</a>
	<li> <a href="http://www.cs.cmu.edu/~ninamf/LGO10/wm.pdf">The strength of weak learnability (Schapire, 1990)</a>
	<li> <a href="https://pdfs.semanticscholar.org/d620/946e24eee13bc3bdd5ceb0f90a3dc4bc4a54.pdf">Boosting a weak learning algorithm by majority (Freund, 1995)</a>
	<li> <a href="http://www.cs.cmu.edu/~ninamf/LGO10/wm.pdf">The weighted majority algorithm (Littlestone, Warmuth, 1992)</a>
	<li> <a href="https://pdfs.semanticscholar.org/5fb5/f7b545a5320f2a50b30af599a9d9a92a8216.pdf">A decision-theoretic generalization of on-line learning and an application to boosting (Freund, Schapire, 1997)</a>
	</ul>

<p>
<li> L06 (Feb 03): Ensemble methods: boosting (statistical perspective) [A] <br>
	<ul>
	<li> <a href="https://www.dropbox.com/s/6mwtmfkf6voj4kf/lecture_06.pdf?dl=0">Scribe note (Weichen Wu)</a>
	<li> <a href="https://mitpress.ublish.com/ereader/7093/?preview=#page/145">Boosting (Foundations of machine learning, chapter 07)</a>
	<li> <a href="http://papers.neurips.cc/paper/1737-potential-boosters.pdf">Potential boosters (Duffy, Helmbold, 1999)</a>	
	<li> <a href="https://papers.nips.cc/paper/1766-boosting-algorithms-as-gradient-descent.pdf">Boosting algorithms as gradient descnet (Mason, Baxter, Bartlett, Frean, 1999)</a>
	<li> <a href="https://projecteuclid.org/download/pdf_1/euclid.aos/1013203451">Greedy function approximation: a gradient boosting machine (Friedman, 2001)</a>	
	</ul>

<p>
<li> L07 (Feb 05): Ensemble methods: boosting (computational considerations, applications) [C, D], guest lecture by Allie <br>
	<ul>
	<li> <a href="https://www.dropbox.com/s/9p59gve9nxu1qfl/lecture_07.pdf?dl=0">Scribe note (Tuhinangshu Choudhury)</a>
	<li> <a href="https://www.dropbox.com/s/9p59gve9nxu1qfl/lecture_07.pdf?dl=0">SpeedBoost: anytime prediction with uniform near-optimality (Grubb, Bagnell, 2012)</a>
	</ul>

<p>
<li> L08 (Feb 10): Ensemble methods: boosting (generalization) [B] <br>
	<ul>
	<li> <a href="https://www.dropbox.com/s/r5u2kfh7uvhw3hn/lecture_08.pdf?dl=0">Scribe note (Rajshekar Das)</a>
	<li> <a href="https://mitpress.ublish.com/ereader/7093/?preview=#page/145">Boosting (Foundations of machine learning, chapter 07)</a>
	<li> <a href="https://projecteuclid.org/download/pdf_1/euclid.aos/1024691352">Boosting the margin: a new explanation for the effectiveness of voting methods</a>
	</ul>

<p>
<li> L09 (Feb 12): Quiz 1  <br>
	<ul>
	<li> Topics: basics (supervised learning), prototype methods (nearest-neighbor methods), predictive inference (conformal prediction), ensemble methods (boosting)
	</ul>

<p>
<li> L10 (Feb 17): Ensemble methods: bagging, random forests [A, B] <br>
	<ul>
	<li> <a href="https://www.dropbox.com/s/w1cwg68uzea1u3s/lecture_10.pdf?dl=0">Scribe note (Andrew Warren)</a>
	<li> <a href="https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf#page=238">Random forests (Hastie, Tibshirani, Friedman, 2017) [Elements of statistical learning, chapter 15]</a>
	<li> <a href="http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf#page=186">Tree-based methods (Jamse, Witten, Hastie, Tibshirani, 2017) [An introduction statistical learning, chapter 08]</a>
	<li> <a href="https://www.stat.berkeley.edu/~breiman/bagging.pdf">Bagging predictors (Leo Breiman, 1994)</a>
	</ul>

<p>
<li> L11 (Feb 19): Variable importance: random forests case study [C, E] <br>
	<ul>
	<li> <a href="https://www.dropbox.com/s/d4y1d6dj3evib7i/lecture_11.pdf?dl=0">Scribe note (Amanda Coston)</a>
	<li> <a href="https://ieeexplore.ieee.org/abstract/document/598994">Random decision forests (Ho, 1995)</a>
	<li> <a href="https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf">Random forests (Breiman, 2001)</a>
	<li> Additional reading: <a href="https://arxiv.org/pdf/1905.03151.pdf">Phase stop permuting features: an explanation and alternatives (Hooker, Mentch, 2019)</a>
	<li> Additional reading: <a href="https://arxiv.org/pdf/2003.03629.pdf">Getting better from worse: augmented bagging and a cautionary tale of variable importance (Mentch, Zhou, 2020)</a>
	</ul>

<p>
<li> L12 (Feb 24): Datapoint importance: Shapley values [C, E] <br>
	<ul>
	<li> <a href="https://www.dropbox.com/s/v0gsrz604vhsyl9/lecture_12.pdf?dl=0">Scribe note (Zeyu Tang)</a>
	<li> <a href="https://www.rand.org/content/dam/rand/pubs/research_memoranda/2008/RM670.pdf">Notes on n-person game -- II: The value of an n-person game (Shapley, 1951)</a>
	<li> <a href="https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions">A unified approach to interpreting model predictions (Lundberg, Lee, 2017)</a>
	<li> <a href="https://arxiv.org/abs/1904.02868">Data Shapley: Equitable valuation of data for machine learning (Ghorbani, Zou, 2019)</a>
	<li> <a href="https://christophm.github.io/interpretable-ml-book/shapley.html">Shapley values (Molnar, 2020) [Interpretable machine learning, chapter 05]</a>
	<li> <a href="https://arxiv.org/pdf/2002.11097.pdf">Problems with Shapley-value-based explanations as feature importance measures (Kumar, Venkatasubramanian, Scheidegger, Friedler, 2020)</a>
	</ul>

<p>
<li> L13 (Feb 26): Ensemble methods: stacking [A, B] <br>
	<ul>
	<li> <a href="https://www.dropbox.com/s/u77y14gjtjdve5d/lecture_13.pdf?dl=0">Scribe note (Don Dennis)</a>
	<li> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0893608005800231">Stacked generalization (Wolpert, 1992)</a>
	<li> <a href="https://statistics.berkeley.edu/sites/default/files/tech-reports/367.pdf">Stacked regressions (Breiman, 1996)</a>

	</ul>
	
<p>
<li> L14 (Mar 02): Predictive inference: jackknife+ [C, E] <br>
	<ul>
	<li> <a href="https://www.dropbox.com/s/1m8z6lex1tb5sly/lecture_14.pdf?dl=0">Scribe note (Max Rubinstein)</a>
	<li> <a href="https://arxiv.org/abs/1905.02928">Predictive inference with the jackknife+ (Barber, Candes, Ramdas, Tibshirani, 2019)</a>
	<li> <a href="https://arxiv.org/abs/2002.09025">Predictive inference is free with the jackknife+after-bootstrap (Kim, Xu, Barber, 2020)</a>

	
	</ul>

<p>
<li> L15 (Mar 04): Predictive inference: leave-one-out [C, E] <br>
	<ul>
	<li> <a href="https://www.dropbox.com/s/knd5epjpa67mwrf/lecture_15.pdf?dl=0">Scribe note (Naveen Basavaraj)</a>
	<li> <a href="https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf#page=238">Model assessment and selection (Hastie, Tibshirani, Friedman, 2017) [Elements of statistical learning, chapter 07]</a>
	<li> <a href="http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf#page=186">Resampling methods (James, Witten, Hastie, Tibshirani, 2017) [An introduction statistical learning, chapter 05]</a>
	</ul>

<p>
<li> L-- (Mar 09): No class (spring break)  <br>


<p>
<li> L-- (Mar 11): No class (spring break) <br>

<p>
<li> L16 (Mar 16): No class (no class due to COVID-19 online preparation) <br>

<p>
<li> L17 (Mar 18): Mid-class reap: (methods/rows) k-nearest-neighbor; boosting; bagging; random forest; stacking; (aspects/columns) algorithms; bias-variance; computation, conformal; (practical) data aspects; explainability, interpretability <br>

	<ul>
	<li> <a href="https://www.dropbox.com/s/xpboq46c2bxhqtl/lecture_17.pdf?dl=0">Scribe note (Lucio Dery)</a>
	</ul>

<p>
<li> L18 (Mar 23):  Quiz 2 <br>

	<ul>
	<li> Topics: variable importance (random forest), data importance (Shapley values), ensemble methods (stacking), predictive inference (jackknife+, leave-one-out)
	</ul>

<p>
<li> L19 (Mar 25): Kernel learning: basics (RKHS intro) <br>

	<ul>
	<li> <a href="https://www.dropbox.com/s/d6knu07mthgckim/lecture_19.pdf?dl=0">Scribe note (Ankur Mallick)</a>
	<li> <a href="https://mitpress.ublish.com/ereader/7093/?preview=#page/105">Kernel methods (Foundations of machine learning, chapter 06)</a>
	<li> <a href="http://mlss.tuebingen.mpg.de/2015/slides/gretton/part_1.pdf">Introduction to RKHS (Gretton, 2015)</a>
	</ul>

<p>
<li> L20 (Mar 30): Kernel learning: basic (RKHS equivalences) <br>

	<ul>
	<li> <a href="https://www.dropbox.com/s/tythg2o8rgxgk2v/lecture_20.pdf?dl=0">Scribe note (Lorenzo Tomaselli)</a>
	<li> <a href="https://mitpress.ublish.com/ereader/7093/?preview=#page/105">Kernel methods (Foundations of machine learning, chapter 06)</a>
	<li> <a href="http://mlss.tuebingen.mpg.de/2015/slides/gretton/part_1.pdf">Mappings of Probabilities to RKHS and applications (Gretton, 2015)</a>
	</ul>

<p>
<li> L21 (Apr 01): Kernel learning: basics (universal/characteristic kernel) <br>

	<ul>
	<li> <a href="https://www.dropbox.com/s/qvk8dw56qz0khg5/lecture_21.pdf?dl=0">Scribe note (Nick Kissel)</a>
	<li> <a href="https://mitpress.ublish.com/ereader/7093/?preview=#page/105">Kernel methods (Foundations of machine learning, chapter 06)</a>
	<li> <a href="http://mlss.tuebingen.mpg.de/2015/slides/gretton/part_1.pdf">Mappings of Probabilities to RKHS and applications (Gretton, 2015)</a>
	
	</ul>

<p>
<li> L22 (Apr 06): Kernel learning: regression, kernel classification (kernel ridge regression, kernel SVM, kernel logistic regression)<br>

	<ul>
	<li> <a href="https://www.dropbox.com/s/v9a85efiyy7g495/lecture_22.pdf?dl=0">Scribe note (Mike Stanley)</a>
	<li> <a href="https://mitpress.ublish.com/ereader/7093/?preview=#page/267">Regression (Foundations of machine learning, chapter 11)</a>
	<li> <a href="http://mlss.tuebingen.mpg.de/2015/slides/gretton/part_3.pdf">Dependence measures using RKHS embeddings (Gretton, 2015)</a>
	</ul>

<p>
<li> L23 (Apr 08): Unsupervised learning: clustering (kernel hierarchical clustering, k-means clustering) <br>

	<ul>
	<li> <a href="">Scribe note (not available, sorry!)</a>
	<li> <a href="https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf#page=504">Unsupervised learning (Hastie, Tibshirani, Friedman, 2017) [Elements of statistical learning, chapter 14]</a>
	<li> <a href="http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf">k-means++: the advantage of careful seeding (Arthur, Vassilvitskii, 2006)</a>
	</ul>
	
<p>
<li> L24 (Apr 13): Unsupervised learning: dimension reduction (PCA, kernel PCA) <br>

	<ul>
	<li> <a href="https://www.dropbox.com/s/q9jvo71kgg0qtl3/lecture_24.pdf?dl=0">Scribe note (Misha Khodak)</a>
	<li> <a href="https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf#page=504">Unsupervised learning (Hastie, Tibshirani, Friedman, 2017) [Elements of statistical learning, chapter 14]</a>
	<li> <a href="http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf#page=381">Resampling methods (James, Witten, Hastie, Tibshirani, 2017) [An introduction statistical learning, chapter 05]</a>
	<li> <a href="https://sebastianraschka.com/Articles/2014_kernel_pca.html">Kernel tricks and nonlinear dimensionality reduction via RBF kernel PCA (Raschka, 2014) </a>
	<li> <a href="https://arxiv.org/pdf/1207.3538.pdf">Kernel Principal Component Analysis and its Applications in Face Recognition and Active Shape Models (Wang, 2012)</a>
	</ul>
	
<p>
<li> L25 (Apr 15):  Unsupervised learning: diemsnsion reduction (stochastic PCA, depp PCA, autoencoders) <br>

	<ul>
	<li> <a href="">Scribe note (not available, sorry!)</a>
	<li> <a href="https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf#page=504">Unsupervised learning (Hastie, Tibshirani, Friedman, 2017) [Elements of statistical learning, chapter 14]</a>
	<li> <a href="http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf#page=381">Unsupervised learning (James, Witten, Hastie, Tibshirani, 2017) [An introduction to statistical learning, chapter 10]</a>
	<li> <a href="https://arxiv.org/pdf/1501.03796.pdf">The Fast Convergence of Incremental PCA (Balsubramani, Dasgupta, Freund, 2015)</a>
	<li> <a href="https://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf">Extracting and Composing Robust Features with Denoising Autoencoders (Vincent, Larochelle, Bengion, Manzagol, 2008) </a>
	<li> <a href="http://proceedings.mlr.press/v27/baldi12a/baldi12a.pdf">Autoencoders, Unsupervised Learning, and Deep architectures (Baldi, 2012) </a>
	<li> <a href="http://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf">Stacked denoising autoencoders: learning useful representations in a deep network with a local denoising criterion (Vincent, Larochelle, Lajoie, Manzagol, 2010) </a>
	<li> <a href="https://arxiv.org/pdf/1904.01277.pdf">A PCA-like autoencoder (Ladjal, Newson, Pham, 2019) </a>

	</ul>

<p>
<li> L26 (Apr 17): Guest lecture by <a href="http://lucasmentch.com/index.html"> Lucas Mentch</a> <br>

	<ul>
	<li> <a href="https://www.dropbox.com/s/h3zi412tm3rafk5/lecture_mentch.pdf?dl=0">Lecture slides</a>
	</ul>
	
<p>
<li> L27 (Apr 20):  <br>

	<ul>
	<li> <a href="https://www.dropbox.com/s/4hgjyo519jp5d5e/lecture_27.pdf?dl=0">Scribe note (Jenn Williams)</a>
	<li> <a href="https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf#page=408">Neural networks (Hastie, Tibshirani, Friedman, 2017) [Elements of statistical learning, chapter 11]</a>
	<li> <a href="http://jmlr.org/papers/volume18/14-546/14-546.pdf">Breaking the curse of dimensionality with convex neural networks (Bach, 2017) </a>
	<li> <a href="http://pages.cs.wisc.edu/~brecht/cs838docs/93.Barron.Universal.pdf">Universal approximation bounds for superpositions of a sigmoid function (Barron, 1993) </a>
	<li> <a href="https://link.springer.com/content/pdf/10.1007/BF02551274.pdf">Approximation of superpositions of a sigmoidal function (Cybenko, 1989) </a>
	<li> <a href="https://www.researchgate.net/profile/Tianping_Chen/publication/3301814_Approximations_of_Continuous_Functionals_by_NN_with_Application_to_Dynamic_Systems/links/02e7e53c380213bf80000000.pdf">Approximations of continuous functionals by neural networks with applications to dynamics systems (Chen, Chen, 1993) </a>
	<li> <a href="https://pdfs.semanticscholar.org/d5cb/b7a8ae0e4ac907515b901d5a3af7f68c98a3.pdf">Universal approximation too nonlinear operators by neural networks with arbitrary activation functions and its applications to dynamical systems (Chen, Chen, 1995) </a>
	<li> <a href="http://proceedings.mlr.press/v49/lee16.pdf">Gradient descent only converges to minimizers (Lee, Simchowitz, Jordan, Recht, 2016) </a>
	<li> <a href="https://papers.nips.cc/paper/8875-first-order-methods-almost-always-avoid-saddle-points-the-case-of-vanishing-step-sizes.pdf">First-order methods almost always avoid saddle points: the case of vanishing step-sizes (Panageas, Piliouras, Wang, 2019) </a>
	<li> <a href="https://arxiv.org/abs/1602.04485">Benefits of depth in neural networks (Telgarsky, 2016) </a>
	</ul>
	
<p>
<li> L28 (Apr 22):  Quiz 3 <br>

	<ul>
	<li> Topics: kernels (basics, regression, classification), unsupervised learning (clustering, PCA, kernel PCA, stochastic PCA, deep PCA, autoencoders)   
	</ul>
	
<p>
<li> L29 (Apr 27): Unsupervised leaerning (ICA, CCA, SDR) <br>

	<ul>
	<li> <a href="https://www.dropbox.com/s/ru28f4mywwgep5i/lecture_29.pdf?dl=0">Scribe note (Zeyu Tang)</a>	
	<li> <a href="https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf#page=504">Unsupervised learning (Hastie, Tibshirani, Friedman, 2017) [Elements of statistical learning, chapter 14]</a>
	<li> <a href="https://www.cs.helsinki.fi/u/ahyvarin/papers/NN00new.pdf">Independent componnet analysis: algorithms and analysis (Hyvarinen, Oja, 2000)</a>
	<li> <a href="https://www.cs.cmu.edu/~tom/10701_sp11/slides/CCA_tutorial.pdf">Canonical correlation: a tutorial (Borga)</a>
	<li> <a href="https://www.jstor.org/stable/2290563?seq=1">Sliced inverse regression for dimension reduction (Ker-Chau Li)</a>
	</ul>
	
<p>
<li> L30 (Apr 29): Calibration and end-class recap <br>
	<ul>
	<li> <a href="https://www.dropbox.com/s/e5za6pnzxokbvmk/lecture_30.pdf?dl=0">Scribe note (Dhivya Eswaran)</a>
	<li> <a href="http://cseweb.ucsd.edu/~elkan/calibrated.pdf">Obtaining calibrated probability estimates from decision trees and naive Bayesian classifiers (Zadrozny, Elkan, 2001)</a>
	<li> <a href="https://vas3k.com/blog/machine_learning/">Machine learning for everyone</a>
	<li> <a href="https://miro.medium.com/max/2000/1*ak2utPSmFTOMoZL3mTgWrg.png">Topics maps (row-focused)</a>
	<li> <a href="https://miro.medium.com/max/1400/1*I69dTEZYndupSeBLgpf5cg.png">Topics map (row-focused)</a>
	<li> <a href="https://i.pinimg.com/originals/7b/6d/b0/7b6db065f26a2c6121d43798c3afeed8.png">Topics map (row-focused)</a>
	</ul>
	
<br>
</ul>

<b>Some textbooks:</b> <br>
<ul>
<li><a href="https://web.stanford.edu/~hastie/ElemStatLearn/">The Elements of Statistical Learning: Data Mining, Inference and Prediction.</a> Trevor Hastie, Robert Tibshirani, Jerome Friedman.<br>
<li><a href="http://faculty.marshall.usc.edu/gareth-james/ISL/">An Introduction to Statistical Learning: With Applications in R.</a> Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani.<br>
<li><a href="https://cs.nyu.edu/~mohri/mlbook/">Foundations of Machine Learning.</a> Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.<br><br>
</ul>


<b>References to catch up on prerequisites:</b>
<ul>
  <li><a href=https://www.youtube.com/channel/UCu8Pv6IJsbQdGzRlZ5OipUg/videos>Videos of Larry Wasserman's 36-705 course</a><br>
  <li><a href="http://www.cs.cmu.edu/~zkolter/course/linalg/index.html">Linear algebra review</a>, videos by Zico Kolter<br>
  <li><a href="https://www.youtube.com/channel/UC7gOYDYEgXG1yIH_rc2LgOw/playlists">Real analysis, calculus, and more linear algebra</a>, videos by Aaditya Ramdas <br>
  <li><a href="prerequisite_topics.pdf">Convex optimization prequisites review</a> from Spring 2015 course, by Nicole Rafidi<br>
  <li>See also Appendix A of <a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">Boyd and Vandenberghe (2004) for general mathematical review</a><br><br>
</ul>
	  
<b>Potentially useful resources available online (not necessarily verified by the course staff):</b>
<ul>
  <li><a href="https://github.com/josephmisiti/awesome-machine-learning/blob/master/books.md">A list of machine learning books</a><br>
  <li><a href="http://ciml.info/">A course in machine learning</a><br>
   <li><a href="https://christophm.github.io/interpretable-ml-book/">Interpretable Machine Learning</a><br>
   <li><a href="https://fairmlbook.org/">Fairness and machine learning</a><br><br>
</ul>

</body>

</html>
