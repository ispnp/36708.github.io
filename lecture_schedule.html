<b>Lectures:</b><br>

<ul>

<p>
<li> L01 (Jan 13): Introduction <br>

	<ul>
	<li> <a href="https://www.dropbox.com/s/nedypj53c98c7tf/Syllabus.pdf?dl=0">Course syllabus</a>
	</ul>

<p>	
<li> L02 (Jan 15): Basics of supervised learning: regression, classification [A, B, C, D, E] <br>

	<ul>
	<li> <a href="https://www.dropbox.com/s/lz27mdj2a9a1lj7/lecture_02.pdf?dl=0">Scribe note (Allie Del Giorno)</a>
	<li> <a href="https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf#page=20">Overview of supervised learning (Hastie, Tibshirani, Friedman, 2017) [Elements of statistical learning, chapter 01]</a>
	<li> <a href="http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf#page28">Statistical learning (Jamse, Witten, Hastie, Tibshirani, 2017) [An introduction statistical learning, chapter 02]</a>
	
	
	</ul>

<p>
<li> L-- (Jan 20): No class (MLK day) <br>


<p>
<li> L03 (Jan 22): Nearest-neighbor methods: k-nn regression and classification [B] <br>

	<ul>
	<li> <a href="https://www.dropbox.com/s/swfsro95xmdfuvr/lecture_03.pdf?dl=0">Scribe note (Yunhan Wen)</a>
	<li> <a href="https://link.springer.com/book/10.1007/978-3-319-25388-6">Lectures on the nearest neighbor method (Blau, Devroye, 2015)</a>
	<li> <a href="https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf#page=478">Prototype methods and nearest-neighbors (Hastie, Tibshirani, Friedman, 2017) [Elements of statistical learning, chapter 13]</a>
	<li> <a href="https://ieeexplore.ieee.org/abstract/document/1053964">Nearest neighbor pattern classification (Cover, Hart, 1967)</a>
	<li> <a href="https://projecteuclid.org/euclid.aos/1176347757">Kernel and nearest-neighbor estimation of a conditional quantile (Bhattacharya, Gangopadhyay, 1990)</a>
	</ul>

<p>
<li> L04 (Jan 27): Predictive inference: conformal prediction [C, E] <br>

	<ul>
	<li> <a href="https://www.dropbox.com/s/5grfh5jwobmmjg2/lecture_04.pdf?dl=0">Scribe note (Ian Waudby-Smith)</a>
	<li> <a href="http://jmlr.csail.mit.edu/papers/volume9/shafer08a/shafer08a.pdf">Conformal prediction (Vovk, 2005) [Algorithmic learning in a random world, chapter 02]</a>
	<li> <a href="http://jmlr.csail.mit.edu/papers/volume9/shafer08a/shafer08a.pdf">A tutorial on conformal prediction (Shafer, Vovk, 2008)</a>
	<li> <a href="https://arxiv.org/pdf/1604.04173.pdf">Distribution-free predictive inference for regression (Lei, G'Sell, Rinaldo, Tibshirani, Wasserman, 2017)</a>
	</ul>

<p>
<li> L05 (Jan 29): Ensemble methods: boosting (game-theoretic perspective) [A] <br>
	<ul>
	<li> <a href="https://www.dropbox.com/s/rtom5jhdpzltvw4/lecture_05.pdf?dl=0">Scribe note (Sasha Podkopaev)</a>
	<li> <a href="https://mitpress.ublish.com/ereader/7093/?preview=#page/145">Boosting (Mohri, Rostamizadeh, Talwalkar, 2018) [Foundations of machine learning, chapter 07]</a>
	<li> <a href="http://www.cs.cmu.edu/~ninamf/LGO10/wm.pdf">The strength of weak learnability (Schapire, 1990)</a>
	<li> <a href="https://pdfs.semanticscholar.org/d620/946e24eee13bc3bdd5ceb0f90a3dc4bc4a54.pdf">Boosting a weak learning algorithm by majority (Freund, 1995)</a>
	<li> <a href="http://www.cs.cmu.edu/~ninamf/LGO10/wm.pdf">The weighted majority algorithm (Littlestone, Warmuth, 1992)</a>
	<li> <a href="https://pdfs.semanticscholar.org/5fb5/f7b545a5320f2a50b30af599a9d9a92a8216.pdf">A decision-theoretic generalization of on-line learning and an application to boosting (Freund, Schapire, 1997)</a>
	</ul>

<p>
<li> L06 (Feb 03): Ensemble methods: boosting (statistical perspective) [A] <br>
	<ul>
	<li> <a href="https://www.dropbox.com/s/6mwtmfkf6voj4kf/lecture_06.pdf?dl=0">Scribe note (Weichen Wu)</a>
	<li> <a href="https://mitpress.ublish.com/ereader/7093/?preview=#page/145">Boosting (Foundations of machine learning, chapter 07)</a>
	<li> <a href="http://papers.neurips.cc/paper/1737-potential-boosters.pdf">Potential boosters (Duffy, Helmbold, 1999)</a>	
	<li> <a href="https://papers.nips.cc/paper/1766-boosting-algorithms-as-gradient-descent.pdf">Boosting algorithms as gradient descnet (Mason, Baxter, Bartlett, Frean, 1999)</a>
	<li> <a href="https://projecteuclid.org/download/pdf_1/euclid.aos/1013203451">Greedy function approximation: a gradient boosting machine (Friedman, 2001)</a>	
	</ul>

<p>
<li> L07 (Feb 05): Ensemble methods: boosting (computational considerations, applications) [C, D], guest lecture by Allie <br>
	<ul>
	<li> <a href="https://www.dropbox.com/s/9p59gve9nxu1qfl/lecture_07.pdf?dl=0">Scribe note (Tuhinangshu Choudhury)</a>
	<li> <a href="https://www.dropbox.com/s/9p59gve9nxu1qfl/lecture_07.pdf?dl=0">SpeedBoost: anytime prediction with uniform near-optimality (Grubb, Bagnell, 2012)</a>
	</ul>

<p>
<li> L08 (Feb 10): Ensemble methods: boosting (generalization) [B] <br>
	<ul>
	<li> <a href="https://www.dropbox.com/s/r5u2kfh7uvhw3hn/lecture_08.pdf?dl=0">Scribe note (Rajshekar Das)</a>
	<li> <a href="https://mitpress.ublish.com/ereader/7093/?preview=#page/145">Boosting (Foundations of machine learning, chapter 07)</a>
	<li> <a href="https://projecteuclid.org/download/pdf_1/euclid.aos/1024691352">Boosting the margin: a new explanation for the effectiveness of voting methods</a>
	</ul>

<p>
<li> L09 (Feb 12): Quiz 1  <br>
	<ul>
	<li> Topics: basics (supervised learning), prototype methods (nearest-neighbor methods), predictive inference (conformal prediction), ensemble methods (boosting)
	</ul>

<p>
<li> L10 (Feb 17): Ensemble methods: bagging, random forests [A, B] <br>
	<ul>
	<li> <a href="https://www.dropbox.com/s/w1cwg68uzea1u3s/lecture_10.pdf?dl=0">Scribe note (Andrew Warren)</a>
	<li> <a href="https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf#page=238">Random forests (Hastie, Tibshirani, Friedman, 2017) [Elements of statistical learning, chapter 15]</a>
	<li> <a href="http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf#page186">Tree-based methods (Jamse, Witten, Hastie, Tibshirani, 2017) [An introduction statistical learning, chapter 08]</a>
	<li> <a href="https://www.stat.berkeley.edu/~breiman/bagging.pdf">Bagging predictors (Leo Breiman, 1994)</a>
	</ul>

<p>
<li> L11 (Feb 19): Variable importance: random forests case study [C, E] <br>
	<ul>
	<li> <a href="https://www.dropbox.com/s/d4y1d6dj3evib7i/lecture_11.pdf?dl=0">Scribe note (Amanda Coston)</a>
	<li> <a href="https://ieeexplore.ieee.org/abstract/document/598994">Random decision forests (Ho, 1995)</a>
	<li> <a href="https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf">Random forests (Breiman, 2001)</a>
	<li> Additional reading: <a href="https://arxiv.org/pdf/1905.03151.pdf">Phase stop permuting features: an explanation and alternatives (Hooker, Mentch, 2019)</a>
	<li> Additional reading: <a href="https://arxiv.org/pdf/2003.03629.pdf">Getting better from worse: augmented bagging and a cautionary tale of variable importance (Mentch, Zhou, 2020)</a>
	</ul>

<p>
<li> L12 (Feb 24): Datapoint importance: Shapley values [C, E] <br>
	<ul>
	<li> <a href="https://www.dropbox.com/s/v0gsrz604vhsyl9/lecture_12.pdf?dl=0">Scribe note (Zeyu Tang)</a>
	<li> <a href="https://www.rand.org/content/dam/rand/pubs/research_memoranda/2008/RM670.pdf">Notes on n-person game -- II: The value of an n-person game (Shapley, 1951)</a>
	<li> <a href="https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions">A unified approach to interpreting model predictions (Lundberg, Lee, 2017)</a>
	<li> <a href="https://arxiv.org/abs/1904.02868">Data Shapley: Equitable valuation of data for machine learning (Ghorbani, Zou, 2019)</a>
	<li> <a href="https://christophm.github.io/interpretable-ml-book/shapley.html">Shapley values (Molnar, 2020) [Interpretable machine learning, chapter 05]</a>
	<li> <a href="https://arxiv.org/pdf/2002.11097.pdf">Problems with Shapley-value-based explanations as feature importance measures (Kumar, Venkatasubramanian, Scheidegger, Friedler, 2020)</a>
	</ul>

<p>
<li> L13 (Feb 26): Ensemble methods: stacking [A, B] <br>
	<ul>
	<li> <a href="https://www.dropbox.com/s/u77y14gjtjdve5d/lecture_13.pdf?dl=0">Scribe note (Don Dennis)</a>
	<li> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0893608005800231">Stacked generalization (Wolpert, 1992)</a>
	<li> <a href="https://statistics.berkeley.edu/sites/default/files/tech-reports/367.pdf">Stacked regressions (Breiman, 1996)</a>

	</ul>
	
<p>
<li> L14 (Mar 02): Predictive inference: jackknife+ [C, E] <br>
	<ul>
	<li> <a href="https://www.dropbox.com/s/1m8z6lex1tb5sly/lecture_14.pdf?dl=0">Scribe note (Max Rubinstein)</a>
	<li> <a href="https://arxiv.org/abs/1905.02928">Predictive inference with the jackknife+ (Barber, Candes, Ramdas, Tibshirani, 2019)</a>
	<li> <a href="https://arxiv.org/abs/2002.09025">Predictive inference is free with the jackknife+after-bootstrap (Kim, Xu, Barber, 2020)</a>

	
	</ul>

<p>
<li> L15 (Mar 04): Predictive inference: leave-one-out [C, E] <br>
	<ul>
	<li> <a href="https://www.dropbox.com/s/knd5epjpa67mwrf/lecture_15.pdf?dl=0">Scribe note (Naveen)</a>
	<li> <a href="https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf#page=238">Model assessment and selection (Hastie, Tibshirani, Friedman, 2017) [Elements of statistical learning, chapter 07]</a>
	<li> <a href="http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf#page186">Resampling methods (James, Witten, Hastie, Tibshirani, 2017) [An introduction statistical learning, chapter 05]</a>
	</ul>

<p>
<li> L-- (Mar 09): No class (spring break)  <br>


<p>
<li> L-- (Mar 11): No class (spring break) <br>


<p>
<li> L16 (Mar 16): Mid-class reap: (methods/rows) k-nearest-neighbor; boosting; bagging; random forest; stacking; (aspects/columns) algorithms; bias-variance; computation, conformal; (practical) data aspects; explainability, interpretability <br>

	<ul>
	<li> <a href="">Scribe note (Lucio Dery)</a>
	</ul>

<p>
<li> L17 (Mar 18):  Quiz 2 <br>

	<ul>
	<li> Topics: 
	</ul>

<p>
<li> L18 (Mar 23):  <br>

	<ul>
	<li> <a href="">Scribe note (Ankur Mallick)</a>
	<li> References:
	</ul>

<p>
<li> L19 (Mar 25):  <br>

	<ul>
	<li> <a href="">Scribe note (Ankur Mallick)</a>
	<li> References:
	</ul>

<p>
<li> L20 (Mar 30):  <br>

	<ul>
	<li> <a href="">Scribe note (Lorenzo Tomaselli)</a>
	<li> References:
	</ul>

<p>
<li> L21 (Apr 01):  <br>

	<ul>
	<li> <a href="">Scribe note (Nick Kissel)</a>
	<li> References:
	</ul>

<p>
<li> L22 (Apr 06):  <br>

	<ul>
	<li> <a href="">Scribe note (Mike Stanley)</a>
	<li> References:
	</ul>

<p>
<li> L23 (Apr 08):  <br>

	<ul>
	<li> <a href="">Scribe note (Saharsh Agarwal)</a>
	<li> References:
	</ul>

	
<p>
<li> L23 (Apr 13):  <br>

	<ul>
	<li> <a href="">Scribe note (Misha Khodak)</a>
	<li> References:
	</ul>
	
<p>
<li> L24 (Apr 15):  Quiz 3 <br>

	<ul>
	<li> Topics:
	</ul>

<p>
<li> L25 (Apr 20):  <br>

	<ul>
	<li> <a href="">Scribe note (Jenn Williams)</a>
	<li> References:
	</ul>
	
<p>
<li> L26 (Apr 22):  <br>

	<ul>
	<li> <a href="">Scribe note (Mohit Sharma)</a>
	<li> References:
	</ul>
	
<p>
<li> L27 (Apr 27):  <br>

	<ul>
	<li> <a href="">Scribe note (Zeyu Tang)</a>
	<li> References:
	</ul>
	
<p>
<li> L28 (Apr 29):  <br>
	<ul>
	<li> <a href="">Scribe note (Dhivya Eswaran)</a>
	<li> References:
	</ul>
	
<br>
</ul>